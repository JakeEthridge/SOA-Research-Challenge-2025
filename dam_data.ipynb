{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "df = pd.read_excel('tarrodan_dam.csv')\n",
    "\n",
    "print(\"Shape of data:\", df.shape)\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "def clean_assessment(value):\n",
    "    if pd.isna(value) or value == 'undefined':\n",
    "        return 'Not Available'\n",
    "    return value\n",
    "\n",
    "df['Assessment'] = df['Assessment'].apply(clean_assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_date(date_str):\n",
    "    if pd.isna(date_str):\n",
    "        return np.nan\n",
    "    try:\n",
    "        return pd.to_datetime(date_str)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df['Last Inspection Date'] = df['Last Inspection Date'].apply(clean_date)\n",
    "df['Assessment Date'] = df['Assessment Date'].apply(clean_date)\n",
    "\n",
    "print(\"\\nUnique hazard values before cleaning:\")\n",
    "print(df['Hazard'].value_counts())\n",
    "\n",
    "df['Hazard'] = df['Hazard'].replace('Undetermined', 'Low')\n",
    "\n",
    "numeric_cols = ['Height (m)', 'Length (km)', 'Volume (m3)', \n",
    "                'Surface (km2)', 'Drainage (km2)', 'Probability of Failure']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df.to_excel('tarrodan_dam_cleaned.xlsx', index=False)\n",
    "\n",
    "print(\"\\nCleaning Summary:\")\n",
    "print(\"1. Standardized Assessment ratings\")\n",
    "print(\"2. Cleaned date formats\")\n",
    "print(\"3. Handled Undetermined hazard cases\")\n",
    "print(\"4. Converted numeric columns to proper type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('tarrodan_dam.csv')\n",
    "\n",
    "df['Assessment'] = df['Assessment'].fillna('Not Available')\n",
    "df['Assessment'] = df['Assessment'].replace('undefined', 'Not Available')\n",
    "\n",
    "df['Last Inspection Date'] = pd.to_datetime(df['Last Inspection Date'], errors='coerce')\n",
    "df['Assessment Date'] = pd.to_datetime(df['Assessment Date'], errors='coerce')\n",
    "\n",
    "df['Hazard'] = df['Hazard'].replace('Undetermined', 'Low')\n",
    "\n",
    "numeric_cols = ['Height (m)', 'Length (km)', 'Volume (m3)', 'Surface (km2)', 'Drainage (km2)']\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    median_by_region = df.groupby('Region')[col].transform('median')\n",
    "    df[col] = df[col].fillna(median_by_region)\n",
    "\n",
    "loss_cols = ['Loss given failure - prop (Qm)', 'Loss given failure - liab (Qm)', 'Loss given failure - BI (Qm)']\n",
    "for col in loss_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    median_by_hazard = df.groupby('Hazard')[col].transform('median')\n",
    "    df[col] = df[col].fillna(median_by_hazard)\n",
    "\n",
    "df['Years Since Inspection'] = (pd.Timestamp.now() - df['Last Inspection Date']).dt.total_seconds() / (365.25 * 24 * 60 * 60)\n",
    "df['Years Since Inspection'] = df['Years Since Inspection'].fillna(df['Years Since Inspection'].median())\n",
    "\n",
    "df['Total Loss Given Failure'] = df['Loss given failure - prop (Qm)'] + df['Loss given failure - liab (Qm)'] + df['Loss given failure - BI (Qm)']\n",
    "\n",
    "df['Risk Score'] = df['Probability of Failure'] * df['Total Loss Given Failure']\n",
    "\n",
    "df['Hazard_Numeric'] = df['Hazard'].map({'Low': 1, 'Significant': 2, 'High': 3})\n",
    "\n",
    "df.to_excel('tarrodan_dam_cleaned_v2.xlsx', index=False)\n",
    "\n",
    "print(\"Missing values after cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nRisk Score Summary by Region:\")\n",
    "print(df.groupby('Region')['Risk Score'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "df = pd.read_excel('tarrodan_dam_cleaned_v2.xlsx')\n",
    "\n",
    "# Create more sophisticated features\n",
    "df['Age'] = 2025 - pd.to_numeric(df['Year Completed'], errors='coerce')\n",
    "df['Maintenance_Score'] = pd.Categorical(df['Assessment']).codes  # Convert assessment to numeric\n",
    "df['Population_Risk'] = df['Hazard_Numeric'] * (1 / (df['Distance to Nearest City (km)'] + 1))\n",
    "df['Volume_Risk'] = df['Volume (m3)'] * df['Height (m)'] / 1000000  # Normalized volume risk\n",
    "\n",
    "# Separate losses by type for more detailed modeling\n",
    "df['Property_Loss_Weight'] = df['Loss given failure - prop (Qm)'] / df['Total Loss Given Failure']\n",
    "df['Liability_Loss_Weight'] = df['Loss given failure - liab (Qm)'] / df['Total Loss Given Failure']\n",
    "df['Business_Loss_Weight'] = df['Loss given failure - BI (Qm)'] / df['Total Loss Given Failure']\n",
    "\n",
    "features = [\n",
    "    'Height (m)',\n",
    "    'Volume (m3)',\n",
    "    'Hazard_Numeric',\n",
    "    'Surface (km2)',\n",
    "    'Age',\n",
    "    'Maintenance_Score',\n",
    "    'Population_Risk',\n",
    "    'Volume_Risk',\n",
    "    'Property_Loss_Weight',\n",
    "    'Liability_Loss_Weight',\n",
    "    'Business_Loss_Weight'\n",
    "]\n",
    "\n",
    "X = df[features]\n",
    "y = df['Total Loss Given Failure'] * df['Probability of Failure']\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    min_samples_split=20,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"R2 Score: {r2_score(y_test, y_pred):.4f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.4f}\")\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'] = 2025 - pd.to_numeric(df['Year Completed'], errors='coerce')\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())  \n",
    "df['Maintenance_Score'] = pd.Categorical(df['Assessment']).codes\n",
    "df['Volume_Risk'] = df['Volume (m3)'] * df['Height (m)'] / 1000000\n",
    "\n",
    "\n",
    "\n",
    "df['Hazard_Numeric'] = df['Hazard_Numeric'].fillna(df['Hazard_Numeric'].median())\n",
    "df['Distance to Nearest City (km)'] = df['Distance to Nearest City (km)'].fillna(df['Distance to Nearest City (km)'].median())\n",
    "df['Population_Risk'] = df['Hazard_Numeric'] * (1 / (df['Distance to Nearest City (km)'] + 1))\n",
    "\n",
    "\n",
    "df['Property_Loss_Weight'] = df['Loss given failure - prop (Qm)'] / df['Total Loss Given Failure']\n",
    "df['Liability_Loss_Weight'] = df['Loss given failure - liab (Qm)'] / df['Total Loss Given Failure']\n",
    "df['Business_Loss_Weight'] = df['Loss given failure - BI (Qm)'] / df['Total Loss Given Failure']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[features].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    min_samples_split=20,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"R2 Score: {r2_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred):.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cost = 'Total Loss Given Failure'\n",
    "\n",
    "y_cost = df[target_cost]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_cost, test_size=0.2, random_state=42)\n",
    "\n",
    "model_cost = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    min_samples_split=20,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model_cost.fit(X_train, y_train)\n",
    "\n",
    "y_pred_cost = model_cost.predict(X_test)\n",
    "\n",
    "print(\"Cost Model Performance:\")\n",
    "print(f\"R2 Score: {r2_score(y_test, y_pred_cost):.4f}\")\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred_cost):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance for Probability of Failure:\")\n",
    "print(feature_importance)\n",
    "\n",
    "\n",
    "feature_importance_cost = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model_cost.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance for Cost of Failure:\")\n",
    "print(feature_importance_cost)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "print(f\"Cross-Validation MAE for Probability of Failure: {-cv_scores.mean():.4f}\")\n",
    "\n",
    "cv_scores_cost = cross_val_score(model_cost, X, y_cost, cv=5, scoring='neg_mean_absolute_error')\n",
    "print(f\"Cross-Validation MAE for Cost of Failure: {-cv_scores_cost.mean():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "df = pd.read_excel('tarrodan_dam_cleaned_v2.xlsx')\n",
    "\n",
    "print(\"Available columns:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "df['Age'] = 2025 - pd.to_numeric(df['Year Completed'], errors='coerce')\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "\n",
    "df['Maintenance_Score'] = pd.Categorical(df['Assessment']).codes\n",
    "\n",
    "if 'Hazard_Numeric' not in df.columns:\n",
    "    df['Hazard_Numeric'] = df['Hazard'].map({'Low': 1, 'Significant': 2, 'High': 3})\n",
    "\n",
    "df['Population_Risk'] = df['Hazard_Numeric'] * (1 / (df['Distance to Nearest City (km)'].fillna(df['Distance to Nearest City (km)'].median()) + 1))\n",
    "\n",
    "df['Volume_Risk'] = df['Volume (m3)'] * df['Height (m)'] / 1000000\n",
    "\n",
    "df['Property_Loss_Weight'] = df['Loss given failure - prop (Qm)'] / df['Total Loss Given Failure']\n",
    "df['Liability_Loss_Weight'] = df['Loss given failure - liab (Qm)'] / df['Total Loss Given Failure']\n",
    "df['Business_Loss_Weight'] = df['Loss given failure - BI (Qm)'] / df['Total Loss Given Failure']\n",
    "\n",
    "for col in ['Property_Loss_Weight', 'Liability_Loss_Weight', 'Business_Loss_Weight']:\n",
    "    df[col] = df[col].fillna(0)\n",
    "\n",
    "features = ['Height (m)', 'Volume (m3)', 'Hazard_Numeric', 'Surface (km2)', \n",
    "            'Age', 'Maintenance_Score', 'Population_Risk', 'Volume_Risk',\n",
    "            'Property_Loss_Weight', 'Liability_Loss_Weight', 'Business_Loss_Weight']\n",
    "\n",
    "X = df[features]\n",
    "y_prob = df['Probability of Failure']\n",
    "y_cost = df['Total Loss Given Failure']\n",
    "\n",
    "X_train, X_test, y_prob_train, y_prob_test, y_cost_train, y_cost_test = train_test_split(\n",
    "    X, y_prob, y_cost, test_size=0.2, random_state=42)\n",
    "\n",
    "model_prob = GradientBoostingRegressor(n_estimators=200, max_depth=5, min_samples_split=20, \n",
    "                                      learning_rate=0.05, random_state=42)\n",
    "model_prob.fit(X_train, y_prob_train)\n",
    "\n",
    "model_cost = GradientBoostingRegressor(n_estimators=200, max_depth=5, min_samples_split=20, \n",
    "                                      learning_rate=0.05, random_state=42)\n",
    "model_cost.fit(X_train, y_cost_train)\n",
    "\n",
    "y_prob_pred = model_prob.predict(X_test)\n",
    "y_cost_pred = model_cost.predict(X_test)\n",
    "\n",
    "print(\"Failure Probability Model Performance:\")\n",
    "print(f\"R2 Score: {r2_score(y_prob_test, y_prob_pred):.4f}\")\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_prob_test, y_prob_pred):.4f}\")\n",
    "\n",
    "print(\"\\nFailure Cost Model Performance:\")\n",
    "print(f\"R2 Score: {r2_score(y_cost_test, y_cost_pred):.4f}\")\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_cost_test, y_cost_pred):.2f}\")\n",
    "\n",
    "df['Predicted_Failure_Probability'] = model_prob.predict(X)\n",
    "df['Predicted_Failure_Cost'] = model_cost.predict(X)\n",
    "df['Expected_Loss'] = df['Predicted_Failure_Probability'] * df['Predicted_Failure_Cost']\n",
    "\n",
    "total_expected_loss = df['Expected_Loss'].sum()\n",
    "expected_loss_10_years = total_expected_loss * 10\n",
    "\n",
    "print(f\"\\nTotal Expected Loss: ${total_expected_loss:,.2f}\")\n",
    "print(f\"Expected Loss over 10 Years: ${expected_loss_10_years:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "annual_deterioration = 0.05\n",
    "\n",
    "years = 10\n",
    "yearly_probabilities = np.zeros((len(df), years))\n",
    "yearly_losses = np.zeros(years)\n",
    "\n",
    "yearly_probabilities[:, 0] = df['Predicted_Failure_Probability'].values\n",
    "\n",
    "for year in range(1, years):\n",
    "    yearly_probabilities[:, year] = np.minimum(\n",
    "        yearly_probabilities[:, year-1] * (1 + annual_deterioration),\n",
    "        1.0  \n",
    "    )\n",
    "    \n",
    "for year in range(years):\n",
    "    yearly_losses[year] = (yearly_probabilities[:, year] * df['Predicted_Failure_Cost']).sum()\n",
    "\n",
    "discount_rate = 0.03  \n",
    "present_value_losses = yearly_losses / (1 + discount_rate) ** np.arange(years)\n",
    "\n",
    "total_expected_loss_10_years = present_value_losses.sum()\n",
    "\n",
    "print(f\"Year-by-year expected losses:\")\n",
    "for year in range(years):\n",
    "    print(f\"Year {year+1}: ${yearly_losses[year]:,.2f}\")\n",
    "    \n",
    "print(f\"\\nTotal expected loss over 10 years: ${total_expected_loss_10_years:,.2f}\")\n",
    "print(f\"Simple 10-year projection (for comparison): ${yearly_losses[0] * 10:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_failure_prob = df['Predicted_Failure_Probability'].mean() * 100  # Convert to percentage\n",
    "high_risk_threshold = df['Predicted_Failure_Probability'].quantile(0.90)\n",
    "high_risk_dams = df[df['Predicted_Failure_Probability'] > high_risk_threshold]\n",
    "high_risk_count = len(high_risk_dams)\n",
    "high_risk_avg_prob = high_risk_dams['Predicted_Failure_Probability'].mean() * 100  # Convert to percentage\n",
    "\n",
    "print(f\"Average failure probability across all dams: {avg_failure_prob:.2f}%\")\n",
    "print(f\"Number of high-risk dams (top 10%): {high_risk_count}\")\n",
    "print(f\"Average failure probability for high-risk dams: {high_risk_avg_prob:.2f}%\")\n",
    "print(f\"High-risk dams by region:\")\n",
    "print(high_risk_dams['Region'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
