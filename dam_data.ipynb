{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "df = pd.read_excel('tarrodan_dam.csv')\n",
    "\n",
    "print(\"Shape of data:\", df.shape)\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "def clean_assessment(value):\n",
    "    if pd.isna(value) or value == 'undefined':\n",
    "        return 'Not Available'\n",
    "    return value\n",
    "\n",
    "df['Assessment'] = df['Assessment'].apply(clean_assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def clean_date(date_str):\n",
    "    if pd.isna(date_str):\n",
    "        return np.nan\n",
    "    try:\n",
    "        return pd.to_datetime(date_str)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df['Last Inspection Date'] = df['Last Inspection Date'].apply(clean_date)\n",
    "df['Assessment Date'] = df['Assessment Date'].apply(clean_date)\n",
    "\n",
    "print(\"\\nUnique hazard values before cleaning:\")\n",
    "print(df['Hazard'].value_counts())\n",
    "\n",
    "df['Hazard'] = df['Hazard'].replace('Undetermined', 'Low')\n",
    "\n",
    "numeric_cols = ['Height (m)', 'Length (km)', 'Volume (m3)', \n",
    "                'Surface (km2)', 'Drainage (km2)', 'Probability of Failure']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df.to_excel('tarrodan_dam_cleaned.xlsx', index=False)\n",
    "\n",
    "print(\"\\nCleaning Summary:\")\n",
    "print(\"1. Standardized Assessment ratings\")\n",
    "print(\"2. Cleaned date formats\")\n",
    "print(\"3. Handled Undetermined hazard cases\")\n",
    "print(\"4. Converted numeric columns to proper type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('tarrodan_dam.csv')\n",
    "\n",
    "df['Assessment'] = df['Assessment'].fillna('Not Available')\n",
    "df['Assessment'] = df['Assessment'].replace('undefined', 'Not Available')\n",
    "\n",
    "df['Last Inspection Date'] = pd.to_datetime(df['Last Inspection Date'], errors='coerce')\n",
    "df['Assessment Date'] = pd.to_datetime(df['Assessment Date'], errors='coerce')\n",
    "\n",
    "df['Hazard'] = df['Hazard'].replace('Undetermined', 'Low')\n",
    "\n",
    "numeric_cols = ['Height (m)', 'Length (km)', 'Volume (m3)', 'Surface (km2)', 'Drainage (km2)']\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    median_by_region = df.groupby('Region')[col].transform('median')\n",
    "    df[col] = df[col].fillna(median_by_region)\n",
    "\n",
    "loss_cols = ['Loss given failure - prop (Qm)', 'Loss given failure - liab (Qm)', 'Loss given failure - BI (Qm)']\n",
    "for col in loss_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    median_by_hazard = df.groupby('Hazard')[col].transform('median')\n",
    "    df[col] = df[col].fillna(median_by_hazard)\n",
    "\n",
    "df['Years Since Inspection'] = (pd.Timestamp.now() - df['Last Inspection Date']).dt.total_seconds() / (365.25 * 24 * 60 * 60)\n",
    "df['Years Since Inspection'] = df['Years Since Inspection'].fillna(df['Years Since Inspection'].median())\n",
    "\n",
    "df['Total Loss Given Failure'] = df['Loss given failure - prop (Qm)'] + df['Loss given failure - liab (Qm)'] + df['Loss given failure - BI (Qm)']\n",
    "\n",
    "df['Risk Score'] = df['Probability of Failure'] * df['Total Loss Given Failure']\n",
    "\n",
    "df['Hazard_Numeric'] = df['Hazard'].map({'Low': 1, 'Significant': 2, 'High': 3})\n",
    "\n",
    "df.to_excel('tarrodan_dam_cleaned_v2.xlsx', index=False)\n",
    "\n",
    "print(\"Missing values after cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nRisk Score Summary by Region:\")\n",
    "print(df.groupby('Region')['Risk Score'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "df = pd.read_excel('tarrodan_dam_cleaned_v2.xlsx')\n",
    "\n",
    "# Create more sophisticated features\n",
    "df['Age'] = 2025 - pd.to_numeric(df['Year Completed'], errors='coerce')\n",
    "df['Maintenance_Score'] = pd.Categorical(df['Assessment']).codes  # Convert assessment to numeric\n",
    "df['Population_Risk'] = df['Hazard_Numeric'] * (1 / (df['Distance to Nearest City (km)'] + 1))\n",
    "df['Volume_Risk'] = df['Volume (m3)'] * df['Height (m)'] / 1000000  # Normalized volume risk\n",
    "\n",
    "# Separate losses by type for more detailed modeling\n",
    "df['Property_Loss_Weight'] = df['Loss given failure - prop (Qm)'] / df['Total Loss Given Failure']\n",
    "df['Liability_Loss_Weight'] = df['Loss given failure - liab (Qm)'] / df['Total Loss Given Failure']\n",
    "df['Business_Loss_Weight'] = df['Loss given failure - BI (Qm)'] / df['Total Loss Given Failure']\n",
    "\n",
    "features = [\n",
    "    'Height (m)',\n",
    "    'Volume (m3)',\n",
    "    'Hazard_Numeric',\n",
    "    'Surface (km2)',\n",
    "    'Age',\n",
    "    'Maintenance_Score',\n",
    "    'Population_Risk',\n",
    "    'Volume_Risk',\n",
    "    'Property_Loss_Weight',\n",
    "    'Liability_Loss_Weight',\n",
    "    'Business_Loss_Weight'\n",
    "]\n",
    "\n",
    "X = df[features]\n",
    "y = df['Total Loss Given Failure'] * df['Probability of Failure']\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    min_samples_split=20,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"R2 Score: {r2_score(y_test, y_pred):.4f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.4f}\")\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
